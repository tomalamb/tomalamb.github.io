---
title: 'Epistemic and Aleatoric Uncertainty in Machine Learning'
date: 2024-02-03
collection: posts
permalink: /posts/2024-02-03-epistemic_and_aleatoric_uncertainty.md/
excerpt: 'Uncertainty; Epistemic Uncertainty; Aleatoric Uncertainty; ML.'
---

# Epistemic and Aleatoric Uncertainty

Uncertainty quantification in machine learning (ML) is critical for deploying models in real-world scenarios, for enhancing training efficiency, and for supporting informed decision-making processes. Consider a medical application where a machine learning model predicts whether a patient has cancer from a scan of the patient. Quantifying the model's uncertainty is crucial for determining when to rely on the model’s predictions or when to defer to human experts. Moreover, during the training of ML models, understanding which data points a model is uncertain about and whether this uncertainty stems from not having seen a certain type of data before or from the inherent noise or ambiguity in the data, can guide targeted additional training for best improving a model’s performance.

![Figure 1: Examples of images from the MNSIT dataset consisting of 28 x 28 greyscale images of handwritten digits](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Untitled.png)

Figure 1: Examples of images from the MNSIT dataset consisting of 28 x 28 greyscale images of handwritten digits

In this blog post, we introduce two forms of uncertainty that are particularly important in ML, namely **epistemic** (reducible) and **aleatoric** (irreducible) uncertainty. We aim to illustrate these two types of uncertainty using a classification task within ML as a running  example, and highlight why quantifying and even untangling each type of uncertainty is important within the context of ML.

We will consider the running example of solving a classification problem under a frequentist setting. By "frequentist setting," we refer to the approach of training on a specific dataset to obtain a fixed final model at the end of training . This fixed model can then be used to make predictions on new and unseen data.* 

Imagine that we have a collection of images, each labeled as belonging to one of $K$ distinct classes. The classification task involves using a model, such as a Convolutional Neural Network (CNN) or a Vision Transformer (ViT), to take an image as an input and predict the specific class that the image belongs to. For a more tangible example, we will focus on a classification model trained on the MNIST dataset [1]. This dataset comprises $28 \times 28$ grayscale images of handwritten digits, ranging from $0$ to $9$. Our model, after being trained on the training portion of this dataset, is capable of taking an arbitrary image of a digit from the MNIST test dataset (images it has not seen before), and predicting the digit's class i.e. what digit from $0-9$ the model thinks is shown within the image. 

As can be seen in the Figure 2, the model doesn't just predict the class for an image; it also provides a confidence level or prediction probability for each class, indicating how strongly the model believes the image belongs to a particular class or contains a particular digit. Note that the probabilities over all the classes sums to $1$ which is what we would expect if these class beliefs are to accurately reflect probabilities. We take the class with the highest prediction probability as our model’s prediction of which digit is within the input image.

![Figure 2: Image classification model with class prediction probabilities for an example input image from the MNIST dataset. Cogs image generated by OpenAI’s ChatGPT [2].](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/1706979891559-e453a65e-c977-43b4-90ad-5ce4eefd49c8_1.jpg)

Figure 2: Image classification model with class prediction probabilities for an example input image from the MNIST dataset. Cogs image generated by OpenAI’s ChatGPT [2].

**In future posts we will consider a Bayesian setting where we don’t obtain a single model after training as in the frequentist setting, but we consider a whole distribution of different models. In particular, we use the data observed during training to discern which of these possible models are more likely than others to reflect the underlying function that we’re trying to model. In this setting, we will look at the distribution of possible models as a way of quantifying uncertainty.* 

## What is aleatoric uncertainty?

Aleatoric uncertainty is the first component of uncertainty that we will focus on. In a nutshell, **aleatoric uncertainty** refers to the uncertainty of our model stemming from th**e inherent noise or ambiguity within our data** that **can’t reduced through additional data**. Let us consider two examples to illustrate these points. 

Firstly, consider Figure 2. Here, the image clearly contains the digit $7$. As we can see, the model also predicts this, predicting this image depicts a $7$ with a high likelihood of $85%$$\%$. Here, our model is confident that the image is a $7$ and based on its prediction probabilities, is displaying low uncertainty in its predictions in that it isn’t assigning any significant likelihood to any other class. 

![Figure 3: Model predictions on ambiguous image. Cogs image generated by OpenAI’s ChatGPT [2].](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Screenshot_2024-02-03_at_17.23.35.png)

Figure 3: Model predictions on ambiguous image. Cogs image generated by OpenAI’s ChatGPT [2].

![Figure 4: Model predictions on OOD image of a $\texttt{v}$. Cogs image generated by OpenAI’s ChatGPT [2].](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Screenshot_2024-02-03_at_17.57.34.png)

Figure 4: Model predictions on OOD image of a $\texttt{v}$. Cogs image generated by OpenAI’s ChatGPT [2].

Now consider Figure 3. Here, the model is presented with an image that is rather ambiguous or ‘noisy’. In particular, this digit could realistically be be a $0$ or an $9$. We can see that the model assigns high probabilities to both of these outcomes indicating that it is rather uncertain about which class the image belongs to; the model is indicating that both options are highly plausible. No matter how many clear examples of $0$s and $9$s we or the model see in the data, this is never going to help the model or us as ‘oracles’ determine whether this specific unclear image is definitely a $0$ or a $9$ - there’s always going to be some level of uncertainty in our answer. This illustrates that this is an inherently noisy and ambiguous example for which our uncertainty can’t be reduced by training on more data. So in this case, we would say that the uncertainty in the model is **aleatoric in nature.**  In real-world scenarios, we could consider images taken under varying lighting conditions or data from sensors that contain measurement errors as examples of data that can be noisy and ambiguous.

Focussing on the predictive distribution, that is the different class prediction probabilities, in Figure 3 we see there is quite a spread of probability between to two plausible classes. Indeed the model is assigning high likelihood to both $0$ and $9$. This is sharp contrast to the example presented in Figure 2 of  clear image of a $7$. The model assigns most of its probability mass, which you can think of as its belief budget, to the digit $7$. In some sense the overall distribution or probabilities over class labels is less spread out or in some sense `less random', and the distribution is `more deterministic' or focussed on a particular class. 

A probabilistic tool that we can use to measure the spread of probability in a distribution is the **entropy** of the distribution. Higher/lower entropy indicates a greater/lower spread in the probabilities within a distribution. A maximum entropy distribution in our scenario would assign each class an equal likelihood of being correct, indicating maximum randomness in the mode’s predictions. On the other extreme, a minimum entropy distribution would be where all of the probability is placed on a single outcome, indicating that the model is certain that a particular outcome is true. In this scenario, there is no randomness, its deterministic. The entropy of a distribution gives a quantitative measure of the spread in a distribution between these two extremes. Because of this, entropy can be used to more concretely frame our discussion above concerning the spread of probability in the predictive distribution of a model and thus help to quantify forms of uncertainty. We’ll come back to this concept in our next post. *   

Now consider presenting a model with a $28 \times 28$ pixel image of the letter 'v'. Given that the model has been trained exclusively on images of digits, it has never encountered letters within its training dataset. As illustrated in Figure 4, the model's response to this unfamiliar type of image is to predict a range of possible digits, essentially making guesses without a clear basis. At first glance, one might question if this uncertainty stems from aleatoric sources. However, the image of the letter 'v' is neither noisy nor ambiguous to us; it is perfectly clear and identifiable as we have seen similar types of images before, the model hasn’t. This observation indicates that the uncertainty displayed by the model is not aleatoric in nature, as it does not arise from data noise or inherent ambiguity, but from a lack of experience with certain types of examples which could be reduced with more data. This leads onto a discussion of epistemic uncertainty.

*This discussion of entropy was a little hand-wavy but hopefully gives some intuition of its use in quantifying how spread out the probabilities in a distribution are. Entropy, in the context of information theory, quantifies the amount of uncertainty or surprise associated with a random variable's possible outcomes. In the next post, we’ll start giving more detail about entropy, including giving its definition and some concrete examples of calculating it for several distributions. 

## What is epistemic uncertainty?

We now focus on the second type of uncertainty mentioned in the introduction, **epistemic uncertainty.** Epistemic uncertainty in a nutshell can be thought of as the uncertainty of our model that stems from a lack of knowledge and that can be reduced with additional data. This in contrast to aleatoric uncertainty that is due to noise in the data and cannot be reduced with additional data. 

Consider the example of learning a new language, say French. We will observe a words that we have never seen before like ‘la fraise’. Maybe we can make some guesses on what we think this word could mean that could be extrapolated based on what we know, like we can see that it is a feminine noun. But overall, if we have never seen this word before we would be uncertain and somewhat clueless as to what this word actually translates to in English. However, once we have learn this work, `la fraise - strawberry', we reduce our uncertainty for this and related examples. In particular, we could now recognise the plural form `les fraises' and understand it in context, for example `Est ce-que vous avez des fraises? - Do you have strawberries?'  

Following on from the previous example,  now consider the sentence "Le vol a été facile." for translation in a French language quiz. The word "vol" introduces inherent ambiguity without context, as it could mean either "theft" or "flight," leading to two valid translations: "The theft was easy." or "The flight was easy." This ambiguity exemplifies aleatoric uncertainty, where the challenge isn't due to a lack of knowledge about the word "vol" and its meanings. Instead, it arises from the intrinsic ambiguity (really **polysemy**) of the word "vol" itself, highlighting a situation where uncertainty remains irreducible without additional context even if we were to practice translating many more sentences from French to English. *

Taking the classification example we discussed above, we considered showing our model an image of a $\texttt{v}$ when the model has only every been trained on and has seen images of a digits. This image is not inherently noisy in the sense that we can identify that this is indeed an image containing the letter $\texttt{v}$ as opposed to some other letter or a digit. We call such a point out-of-distribution (OOD) for the model as it comes from a class of images that our model has not seen during training and is therefore unfamiliar with. By subsequently including images of the letter 'v' in the model’s training dataset, we would effectively bridge the model's knowledge gap. This augmentation of the dataset allows the model to learn and recognise new patterns—specifically, the shape and form of 'v'. As a result, the model's previous uncertainty when encountering 'v'-like images diminishes, it will no longer be less familiar with this type of image. This practical step demonstrates how epistemic uncertainty, rooted in the model's unfamiliarity with certain data, is directly reducible by expanding its experience with those very data types. In this case the uncertainty that our model is **exhibiting** is an example **epistemic uncertainty.**  Note that this contrasts with aleatoric uncertainty, where additional data does not alleviate the uncertainty due to the inherent noise or ambiguity in the observations themselves. This is the **main distinction between aleatoric and epistemic uncertainty:  irreducible vs reducible uncertainty**.  Reducing epistemic uncertainty can also involve strategies beyond adding additional dataset examples in order to reduce the uncertainty in a model due to lack of knowledge. Other strategies include integrating domain knowledge or techniques like transfer learning can also help to address the model's knowledge gaps. We refer a reader to the following papers that explain these concepts in more detail **Insert citation.**

We've illustrated the concept of epistemic uncertainty using an example where a model displays a wide spread in its predictive distribution, signalling uncertainty about the classification of a specific image because it is unfamiliar with a type of image it is trying to classify. However, in the frequentist setting that we are considering where we get a single fixed model after training, such models often face challenges with overfitting to the training data (which one can think of as memorising the data it is trained on and asusming all future data should follow the same patterns) and overconfidence (where a model often makes unjustifiable high probability predictions). This leads to difficulties in generalising to unseen data, with models sometimes assigning very high probabilities to certain classes inaccurately. Thus, for out-of-distribution (OOD) points, relying solely on the spread of the predictive distribution across classes, or more specifically on predictive entropy, becomes problematic. Ideally, we want models to indicate uncertainty when encountering images or data types they haven't seen before. One approach to mitigate this issue is adopting a Bayesian framework, where instead of committing to specific models, we encapsulate our beliefs within the models themselves. This method offers a principled way to manage uncertainty. We will delve into the Bayesian setting in more detail in our next discussion.

*Taking about uncertainty in language can be tricky. Take this example, if we were to allow for additional context to clarify ambiguous translation, then this uncertainty would now be more akin to epistemic uncertainty due to being reducible - for example if we were told that this is in the general context of someone arriving at an airport. For further reading, the following papers discuss the interplay between language, clarifications and uncertainty. 

## Why are these two concepts important?

Why might being able to quantify these two types of uncertainties be important practically to us as ML practitioners when say training models ? In practice why is being able to model these two types of uncertainty is important in training models? Consider that we are training our model with a limited dataset of examples and with limited compute budget, so that we don’t just have a huge dataset and many GPUs to throw at training a model. Being sample efficient is therefore very important. Selecting which samples to show the model that are going to lead to the most improvement is therefore vital.  Identifying a point that the model hasn’t seen before is therefore very important. For example, consider if we’ve only shown the model images of the digits $0-8$ from MNIST but it has never seen a $9$ before, then showing the model a $9$ would mean the model is highly epistemically uncertain about this image and we can reduce our model’s overall uncertainty by showing it a $9$ so that the model is less uncertain about these examples going forward. 

This example may seem slightly adhoc but lets consider a very relevant scenario at the moment which is fine-tuning large language models (LLMs). A popular class of modern LLMs are autoregressive model which are training to predict the next token based on the previous tokens which are often trained with large amounts of unlabelled text in unsupervised setting. We can leverage the expressivity and broadness of these pre-trained models to solve alternative tasks where we may have less data for example in sentiment analysis where we aim to classify say whether a text is generally positive,  negative or neutral. To fine-tune these models with modern methods such as LoRA are still extremely expensive and therefore it is important that we feed it the most important data that will lead to the best downstream performance. Consider the ambiguous the text `Interesting experience, wasn't it?' This is naturally ambiguous as as to whether this is positive or negative sentiment without additional context. Moreover, having additional examples of positive an negative sentiment examples isn't necessarily going to help the model be more certain on the sentiment analysis of this question. In this case, our uncertainty is aleatoric in nature and therefore, training and including such examples are not going to help improve the performance of our model on the downstream sentiment analysis task. It would be better to focus on reducing our knowledge ie.e. epistmeic uncertainty on examples that maybe our model isn't sure of because it hasn't seen such types fo examples before - these examples will lead to the most improvements. Hence, the ability to quantify both types of uncertainty and disentangle them is very import in this context. 

## Just to finish

We hope that the discussion on epistemic and aleatoric uncertainty, their distinctions, and the importance of quantifying them has been a useful introduction to the general area. However, we have only scratched on the topic of uncertainty and its quantification in ML. A few points that are worth keeping in mind going forward:

1. **Calibration**: Our conversation touched upon predictive probabilities and how they can be used in quantifying some form of uncertainty. However, for these to be genuinely informative, they must be well-calibrated. Calibration ensures that a model's predicted probabilities accurately reflect the likelihood of an event occurring. For instance, if a model assigns a $70\%$ probability to a specific classification, ideally, $70\%$ of those predictions should be correct. This aspect of model reliability, where predicted probabilities match observed outcomes, is crucial for making meaningful predictions and reasoning with the associated class probabilities for instance in uncertainty quantification, especially within safety-critical contexts like healthcare.
2. **Quantifying Epistemic Uncertainty**: We've also noted the challenges in identifying out-of-distribution (OOD) points due to models' tendency towards overconfidence and their struggles with generalisation in a frequentist setting. This raises the question: How can we effectively quantify epistemic uncertainty? Various methodologies exist, from more principled Bayesian approaches to other heuristic approaches. These strategies offer different ways to quantify  uncertainty, each with its merits and limitations, which we'll explore in upcoming posts.
3. **Interconnection of Uncertainty Types**: Although aleatoric and epistemic uncertainties might seem distinct based on our initial discussion, they are intrinsically linked. A Bayesian viewpoint, which we will delve into next time, elegantly bridges these two types of uncertainty, offering a single cohesive framework for understanding and quantifying these two types of uncertainty for ML models.

In conclusion, understanding and quantifying uncertainties in ML is a complex but critical for reasoning with and deploying ML models in practice. It enhances model reliability, informs better decision-making, and guides the efficient allocation of resources. As we advance, we'll uncover more about these concepts and the different approaches in quantify and reasoning about them in different ML contexts.

## References

[1] Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research. *IEEE Signal Processing Magazine*, *29*(6), 141–142.

[2] OpenAI. (2024). OpenAI. (2023). *ChatGPT 4* [Software].