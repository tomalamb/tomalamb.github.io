---
title: 'Epistemic and Aleatoric Uncertainty in Machine Learning'
date: 2024-02-03
collection: posts
permalink: /posts/2024-02-03-epistemic_and_aleatoric_uncertainty.md/
excerpt: 'Uncertainty; Epistemic Uncertainty; Aleatoric Uncertainty; ML.'
---

# Epistemic and Aleatoric Uncertainty

Uncertainty quantification in machine learning (ML) is critical for deploying models in real-world scenarios, for enhancing training efficiency, and for supporting informed decision-making processes. Consider a medical application where a machine learning model predicts whether a patient has cancer from a scan of the patient. Quantifying the model's uncertainty is crucial for determining when to rely on the model’s predictions or when to defer to human experts. Moreover, during the training of ML models, understanding which data points a model is uncertain about and whether this uncertainty stems from not having seen a certain type of data before or from the inherent noise or ambiguity in the data, can guide targeted additional training for best improving a model’s performance.

![Figure 1: Examples of images from the MNSIT dataset consisting of 28 x 28 greyscale images of handwritten digits](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Untitled.png)

Figure 1: Examples of images from the MNSIT dataset consisting of 28 x 28 greyscale images of handwritten digits

In this blog post, we introduce two forms of uncertainty that are particularly important in ML, namely **epistemic** (reducible) and **aleatoric** (irreducible) uncertainty. We aim to illustrate these two types of uncertainty using a classification task within ML as a running  example, and highlight why quantifying and even untangling each type of uncertainty is important within the context of ML.

We will consider the running example of solving a classification problem under a frequentist setting. By "frequentist setting," we refer to the approach of training on a specific dataset to obtain a fixed final model at the end of training . This fixed model can then be used to make predictions on new and unseen data.* 

Imagine that we have a collection of images, each labeled as belonging to one of $K$ distinct classes. The classification task involves using a model, such as a Convolutional Neural Network (CNN) [3] or a Vision Transformer (ViT)[4], to take an image as an input and predict the specific class that the image belongs to. For a more tangible example, we will focus on a classification model trained on the MNIST dataset [1]. This dataset comprises $28 \times 28$ grayscale images of handwritten digits, ranging from $0$ to $9$. Our model, after being trained on the training portion of this dataset, is capable of taking an arbitrary image of a digit from the MNIST test dataset (images it has not seen before), and predicting the digit's class i.e. what digit from $0$ to $9$ the model thinks is shown within the image. 

As can be seen in the Figure 2, the model doesn't just predict the class for an image; it also provides a confidence level or prediction probability for each class, indicating how strongly the model believes the image belongs to a particular class or contains a particular digit. Note that the probabilities over all the classes sums to $1$ which is what we would expect if these class beliefs are to accurately reflect probabilities. We take the class with the highest prediction probability as our model’s prediction of which digit is within the input image.

![Figure 2: Image classification model with class prediction probabilities for an example input image from the MNIST dataset. Cogs image generated by OpenAI’s ChatGPT [2].](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Screenshot_2024-02-04_at_22.40.37.png)

Figure 2: Image classification model with class prediction probabilities for an example input image from the MNIST dataset. Cogs image generated by OpenAI’s ChatGPT [2].

**In future posts we will consider a Bayesian setting where we don’t obtain a single model after training as in the frequentist setting, but we consider a whole distribution of different models. In particular, we use the data observed during training to discern which of these possible models are more likely than others to reflect the underlying function that we’re trying to model. In this setting, we will look at the distribution of possible models as a way of quantifying uncertainty.* 

## What is aleatoric uncertainty?

Aleatoric uncertainty is the first component of uncertainty that we will focus on. In a nutshell, **aleatoric uncertainty** refers to the uncertainty of our model stemming from th**e inherent noise or ambiguity within our data** that **can’t reduced through additional data**. Let us consider two examples to illustrate these points. 

Firstly, consider Figure 2. Here, the image clearly contains the digit $7$. As we can see, the model also predicts this, predicting this image depicts a $7$ with a high likelihood of $85%$$\%$. Here, our model is confident that the image is a $7$ and based on its prediction probabilities, is displaying low uncertainty in its predictions in that it isn’t assigning any significant likelihood to any other class. 

![Figure 3: Model predictions on ambiguous image. Cogs image generated by OpenAI’s ChatGPT [2].](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Screenshot_2024-02-04_at_22.41.33.png)

Figure 3: Model predictions on ambiguous image. Cogs image generated by OpenAI’s ChatGPT [2].

Now consider Figure 3. Here, the model is presented with an image that is rather ambiguous or ‘noisy’. In particular, this digit could realistically be be a $0$ or an $9$. We can see that the model assigns high probabilities to both of these outcomes indicating that it is rather uncertain about which class the image belongs to; the model is indicating that both options are highly plausible. No matter how many clear examples of $0$s and $9$s we or the model see in the data, this is never going to help the model or us as ‘oracles’ determine whether this specific unclear image is definitely a $0$ or a $9$ - there’s always going to be some level of uncertainty in our answer. This illustrates that this is an inherently noisy and ambiguous example for which our uncertainty can’t be reduced by training on more data. So in this case, we would say that the uncertainty in the model is **aleatoric in nature.**  In real-world scenarios, we could consider images taken under varying lighting conditions or data from sensors that contain measurement errors as examples of data that can be noisy and ambiguous.

![Figure 4: Model predictions on OOD image of a $\texttt{v}$. Cogs image generated by OpenAI’s ChatGPT [2].](Epistemic%20and%20Aleatoric%20Uncertainty%208009fe1de53f41b8ad7ed4d9fd052264/Screenshot_2024-02-03_at_17.57.34.png)

Figure 4: Model predictions on OOD image of a $\texttt{v}$. Cogs image generated by OpenAI’s ChatGPT [2].

Focussing on the predictive distribution, that is the different class prediction probabilities, in Figure 3 we see there is quite a spread of probability between to two plausible classes. Indeed the model is assigning high likelihood to both $0$ and $9$. This is sharp contrast to the example presented in Figure 2 of  clear image of a $7$. The model assigns most of its probability mass, which you can think of as its belief budget, to the digit $7$. In some sense the overall distribution or probabilities over class labels is less spread out or in some sense `less random', and the distribution is `more deterministic' or focussed on a particular class. 

A probabilistic tool that we can use to measure the spread of probability in a distribution is the **entropy** of the distribution [5]. Higher/lower entropy indicates a greater/lower spread in the probabilities within a distribution. A maximum entropy distribution in our scenario would assign each class an equal likelihood of being correct, indicating maximum randomness in the mode’s predictions. On the other extreme, a minimum entropy distribution would be where all of the probability is placed on a single outcome, indicating that the model is certain that a particular outcome is true. In this scenario, there is no randomness, its deterministic. The entropy of a distribution gives a quantitative measure of the spread in a distribution between these two extremes. Because of this, entropy can be used to more concretely frame our discussion above concerning the spread of probability in the predictive distribution of a model and thus help to quantify forms of uncertainty. We’ll come back to this concept in our next post. *   

Now consider presenting a model with a $28 \times 28$ pixel image of the letter $\texttt{v}$. Given that the model has been trained exclusively on images of digits, it has never encountered letters within its training dataset. As illustrated in Figure 4, the model's response to this unfamiliar type of image is to predict a range of possible digits, essentially making guesses without a clear basis. At first glance, one might question if this uncertainty stems from aleatoric sources. However, the image of the letter $\texttt{v}$ is neither noisy nor ambiguous to us; it is perfectly clear and identifiable as we have seen similar types of images before, the model hasn’t. This observation indicates that the uncertainty displayed by the model is not aleatoric in nature, as it does not arise from data noise or inherent ambiguity, but from a lack of experience with certain types of examples which could be reduced with more data. This leads onto a discussion of epistemic uncertainty.

*This discussion of entropy was a little hand-wavy but hopefully gives some intuition of its use in quantifying how spread out the probabilities in a distribution are. Entropy, in the context of information theory, quantifies the amount of uncertainty or surprise associated with a random variable's possible outcomes. In the next post, we’ll start giving more detail about entropy, including giving its definition and some concrete examples of calculating it for several distributions. 

## What is epistemic uncertainty?

We now focus on the second type of uncertainty mentioned in the introduction, **epistemic uncertainty.** Epistemic uncertainty in a nutshell can be thought of as the uncertainty of our model that stems from a lack of knowledge and that can be reduced with additional data. This in contrast to aleatoric uncertainty that is due to noise in the data and cannot be reduced with additional data. 

Consider the example of learning a new language, say French. We will observe a words that we have never seen before like $\texttt{la fraise}$. Maybe we can make some guesses on what we think this word could mean that could be extrapolated based on what we know, like we can see that it is a feminine noun. But overall, if we have never seen this word before we would be uncertain and somewhat clueless as to what this word actually translates to in English. However, once we have learn this work, $\texttt{la fraise - strawberry}$, we reduce our uncertainty for this and related examples. In particular, we could now recognise the plural form `les fraises' and understand it in context, for example: $\texttt{Est ce-que vous avez des fraises? - Do you have strawberries?}$  

Following on from the previous example,  now consider the sentence: $\texttt{le vol a été facile.}$ for translation in a French language quiz. The word $\texttt{vol}$ introduces inherent ambiguity without context, as it could mean either $\texttt{theft}$ or $\texttt{flight}$, leading to two valid translations: $\texttt{the theft was easy}$ or $\texttt{the flight was easy.}$ This ambiguity exemplifies aleatoric uncertainty, where the challenge isn't due to a lack of knowledge about the word $\texttt{vol}$ and its meanings. Instead, it arises from the intrinsic ambiguity (really **polysemy**) of the word $\texttt{vol}$ itself, highlighting a situation where uncertainty remains irreducible without additional context even if we were to practice translating many more sentences from French to English. *

Taking the classification example we discussed above, we considered showing our model an image of a $\texttt{v}$ when the model has only every been trained on and has seen images of a digits. This image is not inherently noisy in the sense that we can identify that this is indeed an image containing the letter $\texttt{v}$ as opposed to some other letter or a digit. We call such a point out-of-distribution (OOD) for the model as it comes from a class of images that our model has not seen during training and is therefore unfamiliar with. By subsequently including images of the letter $\texttt{v}$ in the model’s training dataset, we would effectively bridge the model's knowledge gap. This augmentation of the dataset allows the model to learn and recognise new patterns—specifically, the shape and form of $\texttt{v}$. As a result, the model's previous uncertainty when encountering $\texttt{v}$-like images diminishes, it will no longer be less familiar with this type of image. This practical step demonstrates how epistemic uncertainty, rooted in the model's unfamiliarity with certain data, is directly reducible by expanding its experience with those very data types. In this case the uncertainty that our model is **exhibiting** is an example **epistemic uncertainty.**  Note that this contrasts with aleatoric uncertainty, where additional data does not alleviate the uncertainty due to the inherent noise or ambiguity in the observations themselves. This is the **main distinction between aleatoric and epistemic uncertainty:  irreducible vs reducible uncertainty**.  Reducing epistemic uncertainty can also involve strategies beyond adding additional dataset examples in order to reduce the uncertainty in a model due to lack of knowledge. Other strategies include integrating domain knowledge or techniques like transfer learning can also help to address the model's knowledge gaps. We refer a reader to the following papers that explain these concepts in more detail **Insert citation.**

In our discussion of epistemic uncertainty, we have noted how models can display a wide spread in their predictive distributions over classes when encountering unfamiliar images, exemplifying uncertainty that is epistemic in nature. However, relying solely on the predictive distribution for quantifying epistemic uncertainty in a frequentist setting presents challenges, particularly due to models' tendency towards overconfidence [6, 7]. This overconfidence—where a model often assigns unjustifiably high probabilities to certain classes—undermines the reliability of using predictive distributions as a straightforward measure of uncertainty. Overconfidence masks the model's true uncertainty level, especially with out-of-distribution (OOD) data, leading to misleadingly high confidence in predictions even when the model confronts entirely new types of data where our model should be naturally be less certain. Moreover, in more real-world scenarios with significantly more complex data like patient scans in medicine, identifying whether a high entropy or spread out predictive distribution is indicative of aleatoric or epistemic uncertainty can be much more difficult even for human experts when inspecting the original labelled image. One principled approach to mitigate the aforementioned issues when relying on the predictive distribution of a model for identifying and disentangling uncertainty is adopting a Bayesian framework, where we model our beliefs about different possible models that could explain the data we observe, rather than committing to one single model at the end of training as in the frequentist setting. We will delve into the Bayesian setting in more detail in our next post!

*Taking about uncertainty in language can be tricky. Take this example, if we were to allow for additional context to clarify ambiguous translation, then this uncertainty would now be more akin to epistemic uncertainty due to being reducible - for example if we were told that this is in the general context of someone arriving at an airport. For further reading, the following papers discuss the interplay between language, clarifications and uncertainty [8,9]. 

## Why are these two concepts important?

Understanding and quantifying the two types of uncertainty—epistemic and aleatoric—is crucial for machine learning (ML) practitioners, especially when training models with limited datasets and compute resources. It is important in such setting to being sample-efficient, allowing your model to learn and improve the most from the least amount of data. For instance, consider a model trained on a dataset of MNIST digits only containing the digits $0$ to $8$. This model  has never seen an image containing the digit $9$. introducing this digit significantly reduces the model's epistemic uncertainty about future $9$ examples by filling a gap in its knowledge. Showing many images of $0$s which the model may already have a good grasp in identifying is going to be a lot less impactful on improving the model than introducing images of never before seen digit of a $9$.

As a more topical example, lets turn to the task of fine-tuning large language models (LLMs), such as autoregressive models trained to predict the next token based on previous ones, often using vast amounts of unlabelled text in an unsupervised setting. These pre-trained models' expressiveness can be harnessed for tasks like sentiment analysis (predicting whether a price of text expresses positive or negative sentiment for example), where data may be scarce. Methods like LoRA, are resource-intensive even while being one of the more efficient fine-tuning approaches for LLMs. Therefore, again we need to look for the most impactful data for fine-tuning for optimal downstream performance. Take the ambiguous text $\texttt{Interesting experience, wasn't it?}$  Without additional context, determining whether this implies a positive or negative sentiment is challenging. More examples of clear positive or negative sentiment won't necessarily clarify the sentiment analysis of such ambiguous statements. This ambiguity represents aleatoric uncertainty, intrinsic to the data, and adding similar examples won't improve model performance on sentiment analysis tasks. Instead, focusing on reducing epistemic uncertainty—uncertainty due to the model's lack of exposure to specific types of examples—promises the most significant performance gains. Therefore, the ability to quantify and differentiate between epistemic and aleatoric uncertainty is essential in this context, enabling more informed decisions about data selection for training and fine-tuning models, especially under constraints of data and computational resources.

## Just to finish

We hope that the discussion on epistemic and aleatoric uncertainty, their distinctions, and the importance of quantifying them has been a useful introduction to the general area. However, we have only scratched on the topic of uncertainty and its quantification in ML. A few points that are worth keeping in mind going forward:

1. **Calibration**: Our conversation touched upon predictive probabilities and how they can be used in quantifying some form of uncertainty. However, for these to be genuinely informative, they must be well-calibrated. Calibration ensures that a model's predicted probabilities accurately reflect the likelihood of an event occurring. For instance, if a model assigns a $70\%$ probability to a specific classification, ideally, $70\%$ of those predictions should be correct. This aspect of model reliability, where predicted probabilities match observed outcomes, is crucial for making meaningful predictions and reasoning with the associated class probabilities for instance in uncertainty quantification, especially within safety-critical contexts like healthcare.
2. **Quantifying Epistemic Uncertainty**: We've also noted the challenges in identifying out-of-distribution (OOD) points due to models' tendency towards overconfidence and their struggles with generalisation in a frequentist setting. This raises the question: How can we effectively quantify epistemic uncertainty? Various methodologies exist, from more principled Bayesian approaches to other heuristic approaches. These strategies offer different ways to quantify  uncertainty, each with its merits and limitations, which we'll explore in upcoming posts.
3. **Interconnection of Uncertainty Types**: Although aleatoric and epistemic uncertainties might seem distinct based on our initial discussion, they are intrinsically linked. A Bayesian viewpoint, which we will delve into next time, elegantly bridges these two types of uncertainty, offering a single cohesive framework for understanding and quantifying these two types of uncertainty for ML models [10].

In conclusion, understanding and quantifying uncertainties in ML is a complex but critical for reasoning with and deploying ML models in practice. It enhances model reliability, informs better decision-making, and guides the efficient allocation of resources. As we advance, we'll uncover more about these concepts and the different approaches in quantify and reasoning about them in different ML contexts.

## References

[1] Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research. *IEEE Signal Processing Magazine*, *29*(6), 141–142.

[2] OpenAI. (2024). OpenAI. (2023). *ChatGPT 4* [Software].

[3] LeCun, Y., Kavukcuoglu, K., & Farabet, C. (2010, May). Convolutional networks and applications in vision. In *Proceedings of 2010 IEEE international symposium on circuits and systems* (pp. 253-256). IEEE.

[4] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020, October). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In *International Conference on Learning Representations*.

[5] Bishop, C. (2006). Pattern recognition and machine learning. *Springer google schola*, *2*, 531-537.

[6] Hendrycks, D., & Gimpel, K. (2016, November). A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. In *International Conference on Learning Representations*.

[7] Liang, S., Li, Y., & Srikant, R. (2018, February). Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks. In *International Conference on Learning Representations*.

[8] Hou, B., Liu, Y., Qian, K., Andreas, J., Chang, S., & Zhang, Y. (2023). Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling. *arXiv preprint arXiv:2311.08718*.

[9] Kuhn, L., & Gal, Y. (2022, June). 1. CLAM: Selective Clarification for Ambiguous Questions with Large Language Models. In *International Conference on Machine Learning*.

[10] Mukhoti, J., Kirsch, A., van Amersfoort, J., Torr, P. H., & Gal, Y. (2023). Deep Deterministic Uncertainty: A New Simple Baseline. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 24384-24394).